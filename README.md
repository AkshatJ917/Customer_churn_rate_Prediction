ğŸ”„ Customer Churn Rate Prediction
An end-to-end Machine Learning project to predict customer churn using the Online Retail II dataset (1M+ transactions).

ğŸ“– Overview
Churn prediction helps businesses identify at-risk customers early and take proactive retention actions. 
This project builds a full ML pipeline â€” from raw data to a final optimized model achieving 91.6% Recall.

ğŸ”„ Workflow
ğŸ§¹ Data Cleaning
      â†“
ğŸ§  RFM Feature Engineering
      â†“
âš–ï¸  SMOTE Class Balancing
      â†“
ğŸ¤– Model Training & Comparison
      â†“
ğŸ¯ Threshold Tuning
      â†“
ğŸ”§ GridSearchCV Hyperparameter Optimization
      â†“
âœ… Final Tuned XGBoost Model

ğŸ¤– Model Comparison
Model                     Recall             F1-Score           ROC-AUC
Logistic Regression        0.841               0.751              0.791
Random Forest              0.689               0.687              0.737
â­ Tuned XGBoost          0.916               0.747              0.793
ğŸ† Winner: Tuned XGBoost @ Threshold 0.40 â€” catches 9 out of 10 at-risk customers!

ğŸ“Š Key Insights from the Final Model
After training, tuning, and evaluating the final Tuned XGBoost model, here are the most important takeaways:

ğŸ¯ High Recall is the priority â€” With a Recall of 91.6%, the model successfully identifies the vast majority of customers likely to churn, minimizing missed opportunities for intervention.
ğŸ’¡ Threshold tuning was a game changer â€” Simply lowering the decision threshold from 0.50 to 0.40 boosted Recall from 79.9% â†’ 91.6%, proving that threshold optimization can be more impactful than hyperparameter tuning alone.
ğŸ” Frequency & Monetary are the strongest churn signals â€” Customers who purchase less frequently and spend lower amounts are significantly more likely to churn.
âš ï¸ Recency causes data leakage â€” Since churn was defined as "no purchase in 90 days", Recency was intentionally excluded from features to prevent the model from cheating.
âš–ï¸ SMOTE prevented bias â€” With a near 50/50 churn split, SMOTE ensured the model didn't over-predict the majority class.
ğŸ”§ GridSearchCV gave marginal but consistent gains â€” Hyperparameter tuning improved ROC-AUC from 0.790 â†’ 0.793 and Recall from 90.5% â†’ 91.6%, confirming the model is well-optimized.

ğŸ“¥ Download the Online Retail II Dataset from the files attached and also checkout the Requirements before working.
